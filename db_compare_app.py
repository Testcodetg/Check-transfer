import json
import hashlib
import pandas as pd
import sqlalchemy as sa
from sqlalchemy.engine import Engine
from typing import List, Tuple, Optional, Dict
import streamlit as st

st.set_page_config(page_title="DB Comparator", layout="wide")

# -------------------------- Utilities --------------------------

@st.cache_data(show_spinner=False)
def make_engine(db_type: str, **kwargs) -> Engine:
    if db_type == "SQL Server":
        driver = kwargs.get("driver") or "ODBC Driver 17 for SQL Server"
        query = {"driver": driver}
        url = sa.engine.URL.create(
            "mssql+pyodbc",
            username=kwargs.get("username"),
            password=kwargs.get("password"),
            host=kwargs.get("host"),
            port=kwargs.get("port"),
            database=kwargs.get("database"),
            query=query,
        )
    else:
        raise ValueError("Only SQL Server is supported in this demo")
    return sa.create_engine(url, pool_pre_ping=True)

@st.cache_data(show_spinner=False)
def list_tables_from_json(json_file: str) -> Dict[str, List[str]]:
    try:
        with open(json_file, "r", encoding="utf-8") as f:
            data = json.load(f)
        return data
    except Exception as e:
        st.error(f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î JSON ‡πÑ‡∏î‡πâ: {e}")
        return {}

@st.cache_data(show_spinner=True, ttl=600)
def read_table(engine: Engine, schema: Optional[str], table: str, limit: Optional[int]) -> pd.DataFrame:
    full = f"{schema}.{table}" if schema else table
    q = f"SELECT * FROM {full}"
    df = pd.read_sql_query(sa.text(q), engine)
    if limit:
        return df.head(limit)
    return df

def compare_tables(df_a: pd.DataFrame, df_b: pd.DataFrame, key_cols: Optional[List[str]] = None) -> Dict[str, pd.DataFrame]:
    res = {}
    meta = {
        "rows_A": len(df_a),
        "rows_B": len(df_b),
        "cols_A": len(df_a.columns),
        "cols_B": len(df_b.columns),
    }
    common_cols = [c for c in df_a.columns if c in set(df_b.columns)]
    df_a_c = df_a[common_cols].copy()
    df_b_c = df_b[common_cols].copy()

    if not key_cols:
        df_a_c["_rowhash"] = df_a_c.apply(lambda row: hashlib.md5("|".join(str(v) for v in row).encode()).hexdigest(), axis=1)
        df_b_c["_rowhash"] = df_b_c.apply(lambda row: hashlib.md5("|".join(str(v) for v in row).encode()).hexdigest(), axis=1)
        key_cols = ["_rowhash"]

    A = df_a_c.set_index(key_cols, drop=False)
    B = df_b_c.set_index(key_cols, drop=False)

    only_in_a = A.loc[~A.index.isin(B.index)]
    only_in_b = B.loc[~B.index.isin(A.index)]

    shared_idx = A.index.intersection(B.index)
    A_shared = A.loc[shared_idx]
    B_shared = B.loc[shared_idx]

    value_cols = [c for c in common_cols if c not in key_cols]
    diffs = []
    for c in value_cols:
        neq = (A_shared[c].astype(str).fillna("") != B_shared[c].astype(str).fillna(""))
        if neq.any():
            tmp = pd.DataFrame({
                "__key__": list(shared_idx),
                "column": c,
                "A": A_shared.loc[neq, c].astype(str).fillna("").values,
                "B": B_shared.loc[neq, c].astype(str).fillna("").values,
            }, index=A_shared.loc[neq].index)
            diffs.append(tmp)
    diff_on_keys = pd.concat(diffs, axis=0) if diffs else pd.DataFrame(columns=["__key__", "column", "A", "B"])

    res["meta"] = pd.DataFrame([meta])
    res["only_in_a"] = only_in_a.reset_index(drop=True)
    res["only_in_b"] = only_in_b.reset_index(drop=True)
    res["diff_on_keys"] = diff_on_keys.reset_index(drop=True)
    return res

# -------------------------- UI --------------------------

st.title("üîç DB Comparator ‚Äî ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á 2 ‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•")
st.caption("‡∏õ‡∏£‡∏±‡∏ö UI ‡πÉ‡∏´‡πâ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡∏ï‡∏≤‡∏°‡∏†‡∏≤‡∏û‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: ‡∏Å‡∏£‡∏≠‡∏Å connection string ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Old/New ‡πÅ‡∏•‡∏∞‡πÇ‡∏´‡∏•‡∏î‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠ table ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå JSON")

# ---------- Helpers for connection string (SQL Server style) ----------
from urllib.parse import quote_plus

def mssql_odbc_url_from_kvp(kvp: str, driver: str = "ODBC Driver 17 for SQL Server") -> sa.engine.URL:
    """Convert ADO-style k=v; strings (server=..;database=..;uid=..;pwd=..) to SQLAlchemy pyodbc URL."""
    parts = {}
    for seg in kvp.split(";"):
        seg = seg.strip()
        if not seg:
            continue
        if "=" in seg:
            k, v = seg.split("=", 1)
            parts[k.strip().lower()] = v.strip()
    server = parts.get("server") or parts.get("data source") or "localhost"
    database = parts.get("database") or parts.get("initial catalog") or "master"
    uid = parts.get("uid") or parts.get("user id")
    pwd = parts.get("pwd") or parts.get("password")
    # Build a pyodbc connection string
    odbc_cs = f"DRIVER={{{driver}}};SERVER={server};DATABASE={database};TrustServerCertificate=yes;"
    if uid:
        odbc_cs += f"UID={uid};"
    if pwd:
        odbc_cs += f"PWD={pwd};"
    return sa.engine.URL.create("mssql+pyodbc", query={"odbc_connect": quote_plus(odbc_cs)})

@st.cache_data(show_spinner=False)
def make_engine_from_connstr(conn_str: str, db_type: str = "SQL Server") -> Engine:
    if db_type == "SQL Server":
        url = mssql_odbc_url_from_kvp(conn_str)
        return sa.create_engine(url, pool_pre_ping=True)
    elif db_type == "SQLite":
        # allow passing path via database=...; in conn_str
        parts = dict(seg.split("=",1) for seg in conn_str.split(";") if "=" in seg)
        return sa.create_engine(sa.engine.URL.create("sqlite", database=parts.get("database","")))
    else:
        raise ValueError("Currently UI ‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö SQL Server/SQLite; ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ MySQL/Postgres ‡πÅ‡∏à‡πâ‡∏á‡πÑ‡∏î‡πâ‡∏Ñ‡∏£‡∏±‡∏ö")

# ---------- JSON schema for table list ----------
# {
#   "master": ["PNM_Zone", "PNM_Province"],
#   "transaction": ["DocHeader", "DocDetail"],
#   "schema": {"PNM_Zone": "dbo", "DocHeader": "hr"}  # (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ) ‡∏£‡∏∞‡∏ö‡∏∏‡∏™‡∏Ñ‡∏µ‡∏°‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ï‡∏≤‡∏£‡∏≤‡∏á
# }

# ---------- Settings Dialog (Popup) ----------
try:
    dialog_api = st.dialog  # Streamlit >= 1.32
except Exception:
    dialog_api = st.experimental_dialog  # Fallback for older versions

@dialog_api("‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Database & ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ Table (JSON)")
def settings_dialog():
    st.write("‡∏Å‡∏£‡∏≠‡∏Å **Connection String** ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Old/New ‡πÅ‡∏•‡∏∞‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå JSON ‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠ Table")
st.session_state["old_server"] = st.text_input("Old server", value=st.session_state["old_server"], key="dlg_old_server")
st.session_state["old_db"] = st.text_input("Old database", value=st.session_state["old_db"], key="dlg_old_db")
st.session_state["old_uid"] = st.text_input("Old uid", value=st.session_state["old_uid"], key="dlg_old_uid")
st.session_state["old_pwd"] = st.text_input("Old pwd", value=st.session_state["old_pwd"], type="password", key="dlg_old_pwd")
    col = st.columns(2)
    with col[0]:
        st.markdown("**Old Database**")
        st.session_state.setdefault("conn_old", "")
        st.session_state.setdefault("old_server", "")
        st.session_state.setdefault("old_db", "")
        st.session_state.setdefault("old_uid", "")
        st.session_state.setdefault("old_pwd", "")
        st.session_state["old_server"] = st.text_input("Old server", value=st.session_state["old_server"], key="dlg_old_server")
        st.session_state["old_db"] = st.text_input("Old database", value=st.session_state["old_db"], key="dlg_old_db")
        st.session_state["old_uid"] = st.text_input("Old uid", value=st.session_state["old_uid"], key="dlg_old_uid")
        st.session_state["old_pwd"] = st.text_input("Old pwd", value=st.session_state["old_pwd"], type="password", key="dlg_old_pwd")
        st.caption("‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: server=TG\\MSSQL2017;database=Cyberhm;uid=sa;pwd=xxxx")
    with col[1]:
        st.markdown("**New Database**")
        st.session_state.setdefault("conn_new", "")
        st.session_state["conn_new"] = st.text_input("server=...;database=...;uid=...;pwd=...", value=st.session_state["conn_new"], key="dlg_conn_new")
        st.caption("‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: server=TG\\MSSQL2017;database=HROpenspaceDB;uid=sa;pwd=xxxx")

    st.markdown("---")
    st.subheader("‡πÑ‡∏ü‡∏•‡πå JSON ‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠ Table")
    up = st.file_uploader("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå JSON", type=["json"], key="dlg_json")
    if up is not None:
        import json
        try:
            st.session_state["tables_cfg"] = json.load(up)
            st.success("‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå JSON ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
        except Exception as e:
            st.error(f"‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå JSON ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ: {e}")

    st.markdown("---")
    default_limit = st.number_input("‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏ï‡πà‡∏≠‡∏Ñ‡∏£‡∏±‡πâ‡∏á", min_value=100, max_value=500000, value=st.session_state.get("default_limit",50000), step=1000, key="dlg_limit")

    save, cancel = st.columns([1,1])
    with save:
        if st.button("üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å & ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠", use_container_width=True):
            try:
                st.session_state["engA"] = make_engine_from_connstr(st.session_state["conn_old"] or "")
                st.session_state["engB"] = make_engine_from_connstr(st.session_state["conn_new"] or "")
                st.session_state["default_limit"] = st.session_state["dlg_limit"]
                st.session_state["_settings_ok"] = True
                st.success("‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Database ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
                st.rerun()
            except Exception as e:
                st.error(f"‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {e}")
    with cancel:
        if st.button("‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å", use_container_width=True):
            st.session_state["_settings_open"] = False
            st.rerun()

# Top bar with gear button
bar_l, bar_r = st.columns([6,1])
with bar_l:
    st.caption("‡πÉ‡∏ä‡πâ‡∏õ‡∏∏‡πà‡∏° ‚öôÔ∏è ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏¥‡∏î‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏ö‡πà‡∏≠‡∏¢)")
with bar_r:
    if st.button("‚öôÔ∏è ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤", use_container_width=True):
        st.session_state["_settings_open"] = True

if st.session_state.get("_settings_open"):
    settings_dialog()

# ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° state
if btn_connect:
    try:
        st.session_state["engA"] = make_engine_from_connstr(conn_old or "")
        st.session_state["engB"] = make_engine_from_connstr(conn_new or "")
        st.success("‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Database ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
    except Exception as e:
        st.error(f"‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {e}")

if json_file is not None:
    try:
        import json
        tables_cfg = json.load(json_file)
        st.session_state["tables_cfg"] = tables_cfg
    except Exception as e:
        st.error(f"‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå JSON ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ: {e}")

# Main body (‡∏≠‡∏¥‡∏á‡∏ï‡∏≤‡∏° JSON)
engA = st.session_state.get("engA")
engB = st.session_state.get("engB")
tables_cfg = st.session_state.get("tables_cfg") or {}

col1, col2 = st.columns(2)
with col1:
    st.subheader("1. ‡πÇ‡∏≠‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Master / ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Master")
with col2:
    st.subheader("2. ‡πÇ‡∏≠‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Transaction / ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Transaction")

master_list = tables_cfg.get("master", [])
tran_list = tables_cfg.get("transaction", [])
schema_map = tables_cfg.get("schema", {})

left, right = st.columns(2)
with left:
    with st.expander("Master (‡∏à‡∏≤‡∏Å JSON)", expanded=True):
        if not master_list:
            st.info("‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠ master ‡πÉ‡∏ô JSON")
        for i, tname in enumerate(master_list):
            label = f"‚Ä¢ {schema_map.get(tname,'')+'.' if schema_map.get(tname) else ''}{tname}"
            if st.button(label, key=f"json_master_{i}"):
                st.session_state["selected"] = (schema_map.get(tname), tname)

with right:
    with st.expander("Transaction (‡∏à‡∏≤‡∏Å JSON)", expanded=True):
        if not tran_list:
            st.info("‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠ transaction ‡πÉ‡∏ô JSON")
        for i, tname in enumerate(tran_list):
            label = f"‚Ä¢ {schema_map.get(tname,'')+'.' if schema_map.get(tname) else ''}{tname}"
            if st.button(label, key=f"json_tran_{i}"):
                st.session_state["selected"] = (schema_map.get(tname), tname)

st.markdown("---")
st.subheader("üß∞ ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö")
st.caption("‡∏Ñ‡∏•‡∏¥‡∏Å‡∏ä‡∏∑‡πà‡∏≠ Table ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å Old/New (A/B) ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô")
compare_mode = st.radio("‡πÇ‡∏´‡∏°‡∏î‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö", ["Row count only", "Primary key & values", "No key (row hash)"], index=1, horizontal=True)
custom_keys = st.text_input("‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡∏µ‡∏¢‡πå‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÉ‡∏´‡πâ‡∏Ñ‡∏±‡πà‡∏ô‡∏î‡πâ‡∏ß‡∏¢ ,)", value="")
preview_limit = st.number_input("‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏ï‡πà‡∏≤‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î (rows)", min_value=10, max_value=5000, value=200, step=10)

selected = st.session_state.get("selected")

if engA and engB and selected:
    sch, tbl = selected
    st.info(f"‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å: {sch+'.' if sch else ''}{tbl} ‚Äî ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å‡∏ó‡∏±‡πâ‡∏á Old(A) ‡πÅ‡∏•‡∏∞ New(B)")

    pkA = get_primary_key(engA, sch or None, tbl)
    pkB = get_primary_key(engB, sch or None, tbl)

    if custom_keys.strip():
        key_cols = [k.strip() for k in custom_keys.split(",") if k.strip()]
    else:
        key_cols = pkA if pkA else (pkB if pkB else None)
        if compare_mode == "Row count only":
            key_cols = []
        elif compare_mode == "No key (row hash)":
            key_cols = None

    with st.spinner("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Old/New ‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö..."):
        dfA = read_table(engA, sch or None, tbl, limit=default_limit)
        dfB = read_table(engB, sch or None, tbl, limit=default_limit)

        if compare_mode == "Row count only":
            meta = pd.DataFrame([{ "table": tbl, "rows_A": len(dfA), "rows_B": len(dfB), "equal?": len(dfA)==len(dfB)}])
            st.dataframe(meta, use_container_width=True)
        else:
            res = compare_tables(dfA, dfB, key_cols=key_cols)
            st.markdown("**‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏• (Meta)**")
            st.dataframe(res["meta"], use_container_width=True)

            st.markdown("**‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÉ‡∏ô Old(A) (‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á)**")
            st.dataframe(res["only_in_a"].head(preview_limit), use_container_width=True)

            st.markdown("**‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÉ‡∏ô New(B) (‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á)**")
            st.dataframe(res["only_in_b"].head(preview_limit), use_container_width=True)

            st.markdown("**‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏Ñ‡πà‡∏≤‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡πÄ‡∏°‡∏∑‡πà‡∏≠ Key ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô (‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á)**")
            st.dataframe(res["diff_on_keys"].head(preview_limit), use_container_width=True)

elif not (engA and engB):
    st.warning("‡πÇ‡∏õ‡∏£‡∏î‡∏Å‡∏î \"‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Database\" ‡πÉ‡∏´‡πâ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏Å‡πà‡∏≠‡∏ô")
else:
    st.info("‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î JSON ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏•‡∏¥‡∏Å‡∏ä‡∏∑‡πà‡∏≠ Table ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö")

st.markdown("---")
st.caption("‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö JSON: { 'master': ['PNM_Zone', ...], 'transaction': ['DocHeader', ...], 'schema': {'PNM_Zone': 'dbo'} }
‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡∏£‡∏∞‡∏ö‡∏ö‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏° Limit ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏£‡∏∞‡∏ö‡∏∏‡∏Ñ‡∏µ‡∏¢‡πå‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥")
